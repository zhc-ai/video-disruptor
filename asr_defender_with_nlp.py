import numpy as np
import librosa
import soundfile as sf
import whisper
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import torch
import jieba

# === éŸ³é¢‘æ‰°åŠ¨æ ¸å¿ƒé€»è¾‘ ===
def load_audio(path, sr=16000):
    audio, _ = librosa.load(path, sr=sr)
    return audio, sr

def save_audio(path, audio, sr=16000):
    sf.write(path, audio, sr)

def strong_perturb_segment(audio, sr):
    t = np.linspace(0, len(audio) / sr, len(audio), endpoint=False)
    hf = 0.02 * np.sin(2 * np.pi * 8000 * t)
    drift = 0.01 * np.sin(2 * np.pi * 20 * t)
    noise = 0.005 * np.random.randn(len(audio))
    perturbed = audio + hf + drift + noise
    perturbed /= np.max(np.abs(perturbed) + 1e-6)
    return perturbed

def time_drift(audio, sr):
    stretch_factor = np.random.uniform(0.95, 1.05)
    audio_stretched = librosa.effects.time_stretch(audio, rate=stretch_factor)
    target_len = len(audio)
    if len(audio_stretched) > target_len:
        return audio_stretched[:target_len]
    else:
        pad = np.zeros(target_len - len(audio_stretched))
        return np.concatenate([audio_stretched, pad])

def process_with_periodic_perturb(input_path, output_path, period_sec=5):
    audio, sr = load_audio(input_path)
    period_len = int(sr * period_sec)
    segments = []

    for i in range(0, len(audio), period_len):
        segment = audio[i:i + period_len]
        if len(segment) < period_len:
            # æœ€åŽä¸è¶³ 5 ç§’çš„æ®µä¸åšæ‰°åŠ¨ï¼Œä¿æŒåŽŸæ ·
            segments.append(segment)
            break

        drifted = time_drift(segment, sr)
        perturbed = strong_perturb_segment(drifted, sr)
        segments.append(perturbed)

    full_audio = np.concatenate(segments)
    save_audio(output_path, full_audio, sr)
    return output_path

# === Whisper è½¬å†™ + NLP ç›¸ä¼¼åº¦ ===
def whisper_transcribe(path):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = whisper.load_model("base").to(device)
    result = model.transcribe(path)
    return result["text"].strip()

def nlp_similarity(text1, text2):
    # ä½¿ç”¨ jieba åˆ†è¯å¹¶ç”¨ç©ºæ ¼è¿žæŽ¥ï¼Œå…¼å®¹ TF-IDF ä¸­æ–‡å¤„ç†
    seg1 = " ".join(jieba.cut(text1))
    seg2 = " ".join(jieba.cut(text2))
    vectorizer = TfidfVectorizer().fit([seg1, seg2])
    tfidf = vectorizer.transform([seg1, seg2])
    score = cosine_similarity(tfidf[0], tfidf[1])[0][0]
    return score

# === ä¸»å‡½æ•° ===
if __name__ == "__main__":
    input_audio_path = "input_audio.wav"
    output_audio_path = "perturbed_output.wav"

    # Step 1: æ‰°åŠ¨å¤„ç†
    process_with_periodic_perturb(input_audio_path, output_audio_path)

    # Step 2: Whisper è½¬å†™
    text_orig = whisper_transcribe(input_audio_path)
    text_pert = whisper_transcribe(output_audio_path)

    # Step 3: NLP ç›¸ä¼¼åº¦
    similarity = nlp_similarity(text_orig, text_pert)
    similarity2 = nlp_similarity("æˆ‘ä»¬éƒ½æ˜¯ä¸­å›½äºº", "æˆ‘ä»¬éƒ½ä¸­å›½äºº")

    # è¾“å‡ºç»“æžœ
    print("ðŸŽ™ï¸ åŽŸå§‹è¯†åˆ«æ–‡æœ¬:", text_orig)
    print("ðŸŒ€ æ‰°åŠ¨åŽè¯†åˆ«æ–‡æœ¬:", text_pert)
    print(f"ðŸ§  NLP ç›¸ä¼¼åº¦: {similarity:.4f}")
    print(f"ðŸ§  NLP ç›¸ä¼¼åº¦: {similarity2:.4f}")