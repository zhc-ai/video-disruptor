import numpy as np
import librosa
import soundfile as sf
import whisper
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import torch
import jieba

# === éŸ³é¢‘æ‰°åŠ¨æ ¸å¿ƒé€»è¾‘ ===

def load_audio(path, sr=16000):
    """
    åŠ è½½éŸ³é¢‘æ–‡ä»¶
    :param path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    :param sr: é‡‡æ ·ç‡ï¼Œé»˜è®¤16000Hz
    :return: è¿”å›éŸ³é¢‘æ•°æ®å’Œé‡‡æ ·ç‡
    """
    audio, _ = librosa.load(path, sr=sr)
    return audio, sr

def save_audio(path, audio, sr=16000):
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    :param path: ä¿å­˜è·¯å¾„
    :param audio: éŸ³é¢‘æ•°æ®
    :param sr: é‡‡æ ·ç‡ï¼Œé»˜è®¤16000Hz
    """
    sf.write(path, audio, sr)

def strong_perturb_segment(audio, sr):
    """
    å¯¹éŸ³é¢‘æ®µæ·»åŠ å¼ºæ‰°åŠ¨
    :param audio: è¾“å…¥éŸ³é¢‘æ®µ
    :param sr: é‡‡æ ·ç‡
    :return: æ·»åŠ æ‰°åŠ¨åçš„éŸ³é¢‘æ®µ
    """
    t = np.linspace(0, len(audio) / sr, len(audio), endpoint=False)  # æ—¶é—´è½´
    hf = 0.02 * np.sin(2 * np.pi * 8000 * t)  # é«˜é¢‘æ‰°åŠ¨
    drift = 0.01 * np.sin(2 * np.pi * 20 * t)  # ä½é¢‘æ¼‚ç§»
    noise = 0.005 * np.random.randn(len(audio))  # éšæœºå™ªå£°
    perturbed = audio + hf + drift + noise  # æ·»åŠ æ‰°åŠ¨
    perturbed /= np.max(np.abs(perturbed) + 1e-6)  # å½’ä¸€åŒ–
    return perturbed

def time_drift(audio, sr):
    """
    å¯¹éŸ³é¢‘è¿›è¡Œæ—¶é—´æ¼‚ç§»
    :param audio: è¾“å…¥éŸ³é¢‘
    :param sr: é‡‡æ ·ç‡
    :return: æ—¶é—´æ¼‚ç§»åçš„éŸ³é¢‘
    """
    stretch_factor = np.random.uniform(0.95, 1.05)  # éšæœºä¼¸ç¼©å› å­
    audio_stretched = librosa.effects.time_stretch(audio, rate=stretch_factor)  # è¿›è¡Œæ—¶é—´ä¼¸ç¼©
    target_len = len(audio)  # ç›®æ ‡é•¿åº¦
    if len(audio_stretched) > target_len:
        return audio_stretched[:target_len]  # æˆªå–åˆ°ç›®æ ‡é•¿åº¦
    else:
        pad = np.zeros(target_len - len(audio_stretched))  # è¡¥é›¶
        return np.concatenate([audio_stretched, pad])  # è¿”å›è¡¥é›¶åçš„éŸ³é¢‘

def process_with_periodic_perturb(input_path, output_path, period_sec=5):
    """
    å¤„ç†éŸ³é¢‘å¹¶æ·»åŠ å‘¨æœŸæ€§æ‰°åŠ¨
    :param input_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
    :param output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
    :param period_sec: æ¯æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
    :return: è¾“å‡ºéŸ³é¢‘è·¯å¾„
    """
    audio, sr = load_audio(input_path)  # åŠ è½½éŸ³é¢‘
    period_len = int(sr * period_sec)  # è®¡ç®—æ¯æ®µçš„æ ·æœ¬é•¿åº¦
    segments = []  # å­˜å‚¨å¤„ç†åçš„éŸ³é¢‘æ®µ

    for i in range(0, len(audio), period_len):
        segment = audio[i:i + period_len]  # åˆ‡åˆ†éŸ³é¢‘æ®µ
        if len(segment) < period_len:
            # æœ€åä¸è¶³ 5 ç§’çš„æ®µä¸åšæ‰°åŠ¨ï¼Œä¿æŒåŸæ ·
            segments.append(segment)
            break

        drifted = time_drift(segment, sr)  # è¿›è¡Œæ—¶é—´æ¼‚ç§»
        perturbed = strong_perturb_segment(drifted, sr)  # æ·»åŠ æ‰°åŠ¨
        segments.append(perturbed)  # å­˜å‚¨æ‰°åŠ¨åçš„éŸ³é¢‘æ®µ

    full_audio = np.concatenate(segments)  # åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ®µ
    save_audio(output_path, full_audio, sr)  # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘
    return output_path

# === Whisper è½¬å†™ + NLP ç›¸ä¼¼åº¦ ===

def whisper_transcribe(path):
    """
    ä½¿ç”¨ Whisper æ¨¡å‹å¯¹éŸ³é¢‘è¿›è¡Œè½¬å†™
    :param path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    :return: è½¬å†™åçš„æ–‡æœ¬
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"  # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
    model = whisper.load_model("base").to(device)  # åŠ è½½ Whisper æ¨¡å‹
    result = model.transcribe(path)  # è½¬å†™éŸ³é¢‘
    return result["text"].strip()  # è¿”å›è½¬å†™æ–‡æœ¬

def nlp_similarity(text1, text2):
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ NLP ç›¸ä¼¼åº¦
    :param text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
    :param text2: ç¬¬äºŒä¸ªæ–‡æœ¬
    :return: ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦åˆ†æ•°
    """
    # ä½¿ç”¨ jieba åˆ†è¯å¹¶ç”¨ç©ºæ ¼è¿æ¥ï¼Œå…¼å®¹ TF-IDF ä¸­æ–‡å¤„ç†
    seg1 = " ".join(jieba.cut(text1))  # å¯¹ç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ†è¯
    seg2 = " ".join(jieba.cut(text2))  # å¯¹ç¬¬äºŒä¸ªæ–‡æœ¬åˆ†è¯
    vectorizer = TfidfVectorizer().fit([seg1, seg2])  # è®¡ç®— TF-IDF
    tfidf = vectorizer.transform([seg1, seg2])  # è½¬æ¢ä¸º TF-IDF çŸ©é˜µ
    score = cosine_similarity(tfidf[0], tfidf[1])[0][0]  # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    return score

# === ä¸»å‡½æ•° ===
if __name__ == "__main__":
    input_audio_path = "input_audio.wav"  # è¾“å…¥éŸ³é¢‘è·¯å¾„
    output_audio_path = "perturbed_output.wav"  # è¾“å‡ºéŸ³é¢‘è·¯å¾„

    # Step 1: æ‰°åŠ¨å¤„ç†
    process_with_periodic_perturb(input_audio_path, output_audio_path)

    # Step 2: Whisper è½¬å†™
    text_orig = whisper_transcribe(input_audio_path)  # è½¬å†™åŸå§‹éŸ³é¢‘
    text_pert = whisper_transcribe(output_audio_path)  # è½¬å†™æ‰°åŠ¨åçš„éŸ³é¢‘

    # Step 3: NLP ç›¸ä¼¼åº¦
    similarity = nlp_similarity(text_orig, text_pert)  # è®¡ç®—è½¬å†™æ–‡æœ¬ç›¸ä¼¼åº¦
    similarity2 = nlp_similarity("æˆ‘ä»¬éƒ½æ˜¯ä¸­å›½äºº", "æˆ‘ä»¬éƒ½ä¸­å›½äºº")  # ç¤ºä¾‹ç›¸ä¼¼åº¦è®¡ç®—

    # è¾“å‡ºç»“æœ
    print("ğŸ™ï¸ åŸå§‹è¯†åˆ«æ–‡æœ¬:", text_orig)  # è¾“å‡ºåŸå§‹æ–‡æœ¬
    print("ğŸŒ€ æ‰°åŠ¨åè¯†åˆ«æ–‡æœ¬:", text_pert)  # è¾“å‡ºæ‰°åŠ¨åæ–‡æœ¬
    print(f"ğŸ§  NLP ç›¸ä¼¼åº¦: {similarity:.4f}")  # è¾“å‡ºç›¸ä¼¼åº¦åˆ†æ•°
    print(f"ğŸ§  NLP ç›¸ä¼¼åº¦: {similarity2:.4f}")  # è¾“å‡ºç¤ºä¾‹ç›¸ä¼¼åº¦åˆ†æ•°